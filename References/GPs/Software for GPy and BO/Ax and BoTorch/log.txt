Bayesian Optimization: From Research to Production with BoTorch & Ax
Abstract

Black-box optimization problems are ubiquitous in many practical settings:
At Facebook, they include AutoML, optimizing ranking policies in large scale online A/B tests,
tuning backend infrastructure,
AI hardware/software co-design,
simulation optimization,
and many others.

The Adaptive Experimentation team at Facebook maintains BoTorch and Ax,
two open-source python libraries for Bayesian Optimization
that are widely used both internally and externally.

In this talk I will explain the origins and the respective goals of BoTorch and Ax,
their features and capabilities,
design principles,
and how they enable the team to conduct novel research
and effectively translate it to production.

I will discuss the tension between catering to different target audiences (researchers as well as practitioners),
and how software engineering is done in practice in an applied research team.
Finally, I will highlight a recent example use-case that leverages both methodological and systems innovations
to efficiently perform automated latency-aware neural architecture search with multi-objective Bayesian optimization.



BoTorch / AX:
Ax: high level black-box BO library
BoTorch: for researchers

BoTorch:
- not only for GPs (should be able to switch)
- PyTorch-like API
Can calculate expectations using MC-sampling, making it tractable to take expectations

Ax:
- Easy to start, less straightforward to configure
- Can use custom models from BoTorch in Ax: ..tutorials/modular_botax.html
- Has a scheduler that takes care of when and where to run experiments
